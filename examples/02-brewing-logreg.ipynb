{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brewing Logistic Regression then Going Deeper\n",
    "\n",
    "While Caffe is made for deep networks it can likewise represent \"shallow\" models like logistic regression for classification. We'll do simple logistic regression on synthetic data that we'll generate and save to HDF5 to feed vectors to Caffe. Once that model is done, we'll add layers to improve accuracy. That's what Caffe is about: define a model, experiment, and then deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './python')\n",
    "import caffe\n",
    "\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthesize a dataset of 10,000 4-vectors for binary classification with 2 informative features and 2 noise features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = sklearn.datasets.make_classification(\n",
    "    n_samples=10000, n_features=4, n_redundant=0, n_informative=2, \n",
    "    n_clusters_per_class=2, hypercube=False, random_state=0\n",
    ")\n",
    "\n",
    "# Split into train and test\n",
    "X, Xt, y, yt = sklearn.cross_validation.train_test_split(X, y)\n",
    "\n",
    "# Visualize sample of the data\n",
    "ind = np.random.permutation(X.shape[0])[:1000]\n",
    "df = pd.DataFrame(X[ind])\n",
    "_ = pd.scatter_matrix(df, figsize=(9, 9), diagonal='kde', marker='o', s=40, alpha=.4, c=y[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn and evaluate scikit-learn's logistic regression with stochastic gradient descent (SGD) training. Time and check the classifier's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a73ed6cecd07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'timeit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu\"# Train and test the scikit-learn SGD logistic regression.\\nclf = sklearn.linear_model.SGDClassifier(\\n    loss='log', n_iter=1000, penalty='l2', alpha=1e-3, class_weight='auto')\\n\\nclf.fit(X, y)\\nyt_pred = clf.predict(Xt)\\nprint('Accuracy: {:.3f}'.format(sklearn.metrics.accuracy_score(yt, yt_pred)))\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/cs/vml2/avahdat/software/Anaconda-2.2.0/release/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2259\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2260\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2261\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2262\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/cs/vml2/avahdat/software/Anaconda-2.2.0/release/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell)\u001b[0m\n",
      "\u001b[1;32m/cs/vml2/avahdat/software/Anaconda-2.2.0/release/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/cs/vml2/avahdat/software/Anaconda-2.2.0/release/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m   1034\u001b[0m             \u001b[0mnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m                 \u001b[0mtime_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m                 \u001b[0mworst_tuning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworst_tuning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/cs/vml2/avahdat/software/Anaconda-2.2.0/release/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Train and test the scikit-learn SGD logistic regression.\n",
    "clf = sklearn.linear_model.SGDClassifier(\n",
    "    loss='log', n_iter=1000, penalty='l2', alpha=1e-3, class_weight='auto')\n",
    "\n",
    "clf.fit(X, y)\n",
    "yt_pred = clf.predict(Xt)\n",
    "print('Accuracy: {:.3f}'.format(sklearn.metrics.accuracy_score(yt, yt_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataset to HDF5 for loading in Caffe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write out the data to HDF5 files in a temp directory.\n",
    "# This file is assumed to be caffe_root/examples/hdf5_classification.ipynb\n",
    "dirname = os.path.abspath('./examples/hdf5_classification/data')\n",
    "if not os.path.exists(dirname):\n",
    "    os.makedirs(dirname)\n",
    "\n",
    "train_filename = os.path.join(dirname, 'train.h5')\n",
    "test_filename = os.path.join(dirname, 'test.h5')\n",
    "\n",
    "# HDF5DataLayer source should be a file containing a list of HDF5 filenames.\n",
    "# To show this off, we'll list the same data file twice.\n",
    "with h5py.File(train_filename, 'w') as f:\n",
    "    f['data'] = X\n",
    "    f['label'] = y.astype(np.float32)\n",
    "with open(os.path.join(dirname, 'train.txt'), 'w') as f:\n",
    "    f.write(train_filename + '\\n')\n",
    "    f.write(train_filename + '\\n')\n",
    "    \n",
    "# HDF5 is pretty efficient, but can be further compressed.\n",
    "comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
    "with h5py.File(test_filename, 'w') as f:\n",
    "    f.create_dataset('data', data=Xt, **comp_kwargs)\n",
    "    f.create_dataset('label', data=yt.astype(np.float32), **comp_kwargs)\n",
    "with open(os.path.join(dirname, 'test.txt'), 'w') as f:\n",
    "    f.write(test_filename + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define logistic regression in Caffe through Python net specification. This is a quick and natural way to define nets that sidesteps manually editing the protobuf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def logreg(hdf5, batch_size):\n",
    "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
    "    n = caffe.NetSpec()\n",
    "    n.data, n.label = L.HDF5Data(batch_size=batch_size, source=hdf5, ntop=2)\n",
    "    n.ip1 = L.InnerProduct(n.data, num_output=2, weight_filler=dict(type='xavier'))\n",
    "    n.accuracy = L.Accuracy(n.ip1, n.label)\n",
    "    n.loss = L.SoftmaxWithLoss(n.ip1, n.label)\n",
    "    return n.to_proto()\n",
    "    \n",
    "with open('examples/hdf5_classification/logreg_auto_train.prototxt', 'w') as f:\n",
    "    f.write(str(logreg('examples/hdf5_classification/data/train.txt', 10)))\n",
    "    \n",
    "with open('examples/hdf5_classification/logreg_auto_test.prototxt', 'w') as f:\n",
    "    f.write(str(logreg('examples/hdf5_classification/data/test.txt', 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to learn and evaluate our Caffeinated logistic regression in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "caffe.set_mode_cpu()\n",
    "solver = caffe.get_solver('examples/hdf5_classification/solver.prototxt')\n",
    "solver.solve()\n",
    "\n",
    "accuracy = 0\n",
    "batch_size = solver.test_nets[0].blobs['data'].num\n",
    "test_iters = int(len(Xt) / batch_size)\n",
    "for i in range(test_iters):\n",
    "    solver.test_nets[0].forward()\n",
    "    accuracy += solver.test_nets[0].blobs['accuracy'].data\n",
    "accuracy /= test_iters\n",
    "\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same through the command line interface for detailed output on the model and solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./build/tools/caffe train -solver examples/hdf5_classification/solver.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at output or the `logreg_auto_train.prototxt`, you'll see that the model is simple logistic regression.\n",
    "We can make it a little more advanced by introducing a non-linearity between weights that take the input and weights that give the output -- now we have a two-layer network.\n",
    "That network is given in `nonlinear_auto_train.prototxt`, and that's the only change made in `nonlinear_solver.prototxt` which we will now use.\n",
    "\n",
    "The final accuracy of the new network should be higher than logistic regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "\n",
    "def nonlinear_net(hdf5, batch_size):\n",
    "    # one small nonlinearity, one leap for model kind\n",
    "    n = caffe.NetSpec()\n",
    "    n.data, n.label = L.HDF5Data(batch_size=batch_size, source=hdf5, ntop=2)\n",
    "    # define a hidden layer of dimension 40\n",
    "    n.ip1 = L.InnerProduct(n.data, num_output=40, weight_filler=dict(type='xavier'))\n",
    "    # transform the output through the ReLU (rectified linear) non-linearity\n",
    "    n.relu1 = L.ReLU(n.ip1, in_place=True)\n",
    "    # score the (now non-linear) features\n",
    "    n.ip2 = L.InnerProduct(n.ip1, num_output=2, weight_filler=dict(type='xavier'))\n",
    "    # same accuracy and loss as before\n",
    "    n.accuracy = L.Accuracy(n.ip2, n.label)\n",
    "    n.loss = L.SoftmaxWithLoss(n.ip2, n.label)\n",
    "    return n.to_proto()\n",
    "    \n",
    "with open('examples/hdf5_classification/nonlinear_auto_train.prototxt', 'w') as f:\n",
    "    f.write(str(nonlinear_net('examples/hdf5_classification/data/train.txt', 10)))\n",
    "    \n",
    "with open('examples/hdf5_classification/nonlinear_auto_test.prototxt', 'w') as f:\n",
    "    f.write(str(nonlinear_net('examples/hdf5_classification/data/test.txt', 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "caffe.set_mode_cpu()\n",
    "solver = caffe.get_solver('examples/hdf5_classification/nonlinear_solver.prototxt')\n",
    "solver.solve()\n",
    "\n",
    "accuracy = 0\n",
    "batch_size = solver.test_nets[0].blobs['data'].num\n",
    "test_iters = int(len(Xt) / batch_size)\n",
    "for i in range(test_iters):\n",
    "    solver.test_nets[0].forward()\n",
    "    accuracy += solver.test_nets[0].blobs['accuracy'].data\n",
    "accuracy /= test_iters\n",
    "\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same through the command line interface for detailed output on the model and solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./build/tools/caffe train -solver examples/hdf5_classification/nonlinear_solver.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean up (comment this out if you want to examine the hdf5_classification/data directory).\n",
    "shutil.rmtree(dirname)"
   ]
  }
 ],
 "metadata": {
  "description": "Use Caffe as a generic SGD optimizer to train logistic regression on non-image HDF5 data.",
  "example_name": "Off-the-shelf SGD for classification",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "priority": 3
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
